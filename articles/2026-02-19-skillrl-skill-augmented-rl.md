# SkillRL：学会从记忆轨迹提炼技能

> **来源**: [arXiv 2602.08234](https://arxiv.org/abs/2602.08234) · UNC-Chapel Hill、芝加哥大学、UCSD、伯克利
> **小红书解读**: [原文链接](http://xhslink.com/o/4ei21T860Qt)

💡 **看点：让 Agent 从"死记硬背轨迹"进化到"学会提炼技能"，token 占用砍掉 90%，性能超越基线 15.3%。**

---

## 问题：原始轨迹记忆的三大瓶颈

现有 Agent 记忆系统直接存储原始交互轨迹，存在三个核心问题：

1. **信息冗余** — 原始轨迹是流水账，大量无关细节占满上下文窗口
2. **噪声干扰** — 失败经验没提炼，Agent 看到的是"我失败了"而不是"为什么失败、下次怎么避免"
3. **无法泛化** — 缺乏抽象能力，在任务 A 学到的经验，到任务 B 完全用不上

## 核心方案：三层递归机制

### 技能蒸馏（Skill Distillation）

成功轨迹 → 战略演示：教师模型识别关键决策点，提取可泛化模式。

失败轨迹 → 失败教训：分析哪一步推理出了错，生成"如果当时这样做会怎样"的反事实建议，转化成"下次遇到类似情况，千万别……"的防护原则。

效果：将冗长轨迹蒸馏为简洁技能，实现 10-20 倍 token 减少。

### 分层技能库（Hierarchical SkillBank）

有些技能是通用策略，有些则是场景特定的，混在一起检索效率低。

- **通用技能** — 跨任务通用策略，如系统探索、状态验证
- **任务特定技能** — 针对特定场景的启发式方法
- **检索** — 通过语义相似度匹配，自适应获取相关技能

### 三阶段动态演化

1. **冷启动 SFT** — 教师模型生成技能增强轨迹，基础模型学习如何使用技能
2. **RL 训练中演化** — 每 5 步验证后，针对失败任务类别生成新技能或改进现有技能
3. **GRPO 优化** — 策略与技能库共同演化，用 KL 惩罚确保 Agent 在学新东西时不会把旧技能丢掉

## 实验结果

**ALFWorld：** 总体成功率 89.9% vs 77.6%（+12.3%），复杂任务 Cool +23.0%，超越 GPT-4o +41.9%

**WebShop：** 成功率 72.7% vs 66.1%（+6.6%），平均得分 85.2 vs 79.3

**搜索增强 QA：** 平均准确率 47.1% vs 43.1%（+4.0%），多跳推理 Bamboogle 达 73.8%，超越 EvolveR +19.4%

## 与我们记忆系统的关联

SkillRL 的思路和我们的记忆图谱系统有很多共鸣：

- **技能蒸馏 ↔ recall.js commit** — 都是从原始经验中提取可复用的知识
- **分层技能库 ↔ graph.json clusters** — 都用分层结构组织知识，通用 vs 特定
- **失败教训 ↔ failure-tracker.js** — 都从失败中学习，生成防护规则
- **动态演化 ↔ sleep.js + dream.js** — 都有离线整合和知识进化机制

关键差异：SkillRL 用 RL 训练让技能库和策略共同演化，我们目前是规则驱动的。这个方向值得探索。
